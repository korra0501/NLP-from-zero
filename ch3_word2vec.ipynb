{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37164bitbasecondac1ab1b3ac90d4e2981ba357c16f579f5",
   "display_name": "Python 3.7.1 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec\n",
    "이 장에서 다룰 것도 마찬가지로 단어의 분산표현이다. 여기서는 '추론에 기반한 방법'에 대해 알아볼 것이다.\n",
    "먼저, word2vec을 실제로 코드로 짜보자!\n",
    "\n",
    "## 추론 기반의 방법과 신경망\n",
    "단어를 벡터로 표현한 방법 중, 성공한 것은 '카운트 기반 방법'과 '추론 기반 방법'이다. 이 두가지 방법의 배경에는 분배가설이 있다.\n",
    "\n",
    "### 카운트 기반의 방법의 문제점\n",
    "카운트 기반의 문제점은, 역시 어휘가 많아질 때 늘어난다. 어휘량이 100만개라고 할 때, 이 방식으로는 100만 x 100만의 거대한 행렬을 만들게 된다. 이 때, 이렇게 커다란 행렬에 대해 특이값분해를 하는 것은 현실적으로 불가능하다.\n",
    "\n",
    "카운트 기반 방법은 코퍼스 전체의 통계 데이터(cocurrence matrix나 ppmi 등)을 이용해, 1회의 처리(SVD 등)으로 단어의 분산표현을 얻는다. 반면, 추론 기반 방법은 미니배치를 이용한 학습을 진행한다. 따라서 연산의 부하가 덜 걸리게 된다.\n",
    "\n",
    "### 추론 기반의 방법 개요\n",
    "추론 기반은 말 그대로 문맥이 주어졌을 때, 중간에 구멍을 뚫어놓고 어떤 단어가 출현할지 추측하는 것이다. 이런 추측을 거듭하여, 단어의 출현 패턴을 학습한다.\n",
    "\n",
    "우리는 앞으로 신경망으로 단어를 처리할 것이다. 그러나, 'you'나 'will'같은 단어를 컴퓨터가 그대로 처리하지는 못한다. 신경망에 통과시키기 위해서는, 벡터로 변환할 필요가 있다. 이 방법 중 하나는 단어를 one-hot vector로 변환하는 것이다.\n",
    "\n",
    "따라서, 모든 단어를 one-hot vector로 변환하면 이 벡터의 집합은 신경망을 구성하는 다양한 '레이어'에 의해 처리할 수 있다.\n",
    "그렇다면 예를 들어보자. \n",
    "\n",
    "아래 예에서 c와 W의 내적은, 단순히 W의 행벡터를 \"끄집어 내는\" 것에 지나지 않는다. 비효율적이다. 그래서 ch4에서는 이를 개량한 코드를 작성할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[-0.78644231 -1.06419704 -1.88519838]]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# bias는 생략\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]]) # 입력층. 단어ID에 해당하는 원소는 1이고, 나머지는 0인 ONE-HOT VECTOR.\n",
    "W = np.random.randn(7,3) # weight\n",
    "h = np.dot(c, W) # 중간 노드\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 2.12367396 -0.72416965 -1.58687127]]\n"
    }
   ],
   "source": [
    "# Matmul layer를 사용해서 다시 쓸 수 있다.\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]]) # 입력층. 단어ID에 해당하는 원소는 1이고, 나머지는 0인 ONE-HOT VECTOR.\n",
    "W = np.random.randn(7,3) # weight\n",
    "layer = MatMul(W)\n",
    "h = layer.forward(c)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW(Continuous Bag-of-words)\n",
    "이제 신경망을 만들어 보자. word2vec에서 쓰이는 CBOW(Continuous Bag-of-words)라고 쓰이는 모델을 사용할 것이다.\n",
    "\n",
    "사실 word2vec이라는 용어는 모델이 아니라, 프로그램의 이름이다. 정확히는 CBOW 모델과 skip-gram 모델이라는 두 가지 모델이 word2vec에 사용되는 신경망이다.\n",
    "\n",
    "CBOW 모델은, 문맥에서 타겟(가운데 단어)을 추측하는 것을 목적으로 한 신경망이다. CBOW 모델이 입력받는 것은 문맥(컨텍스트)이다. 이 컨텍스트는 `['i', 'love', 'you']`와 같은 단어의 리스트로 표현된다. 이를 one-hot 표현으로 바꿔 CBOW 모델이 처리할 수 있게 변환할 것이다.\n",
    "\n",
    "CBOW의 입력층은 문맥의 단어 개수에 따라 결정된다. 만약 문맥으로 N개의 단어를 다룬다면 입력층은 N개가 된다. 입력층에서 중간층의 변환은, 같은 Affine layer(weight: Win)에 의해 이루어진다. 그리고 중간층에서 출력층의 변환은 다른 Affine layer(weight: Wout)에 의해 이루어진다.\n",
    "\n",
    "중간층에 있는 뉴런은, 각 입력층의 변환값의 '평균'이다. 이 뉴런들의 가중치 자체가 단어의 분산표현이 된다는 것에 주의하자. 학습을 거듭할수록 문맥으로부터 출현하는 단어를 잘 추측하도록 각 단어의 분산표현이 갱신된다. 또한, 중간층의 뉴런 수를 입력층보다 줄이는 것이 중요하다. 단어를 예측할 때 필요한 정보를 컴팩트하게 저장할 필요가 있기 때문이다.\n",
    "\n",
    "또한, 출력층 뉴런들은 각 단어에 대응한다. 즉, 츌력층의 뉴런은 각 단어의 '스코어'이며, 이 스코어가 높을수록 대응하는 단어의 출현확률도 높아진다. 또한 스코어에 Softmax 함수를 적용하여 '확률'을 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[-1.25504528  2.0576921  -1.02357845 -0.85518541  0.32020726  1.85893387\n   0.1182449 ]]\n"
    }
   ],
   "source": [
    "# CBOW Model의 추론처리 방법\n",
    "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
    "\n",
    "# Weight Init\n",
    "W_in = np.random.randn(7, 3)\n",
    "W_out = np.random.randn(3, 7)\n",
    "\n",
    "# 레이어 생성\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# Forward Propagation\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = (h0 + h1) / 2 # 레이어 2개를 더했으므로, 2로 나누어서 평균치를 구한다.\n",
    "s = out_layer.forward(h)\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW 모델의 학습\n",
    "CBOW모델은 코퍼스의 단어 출현패턴을 학습할 뿐이다. 따라서, 코퍼스가 달라지면, 학습으로 얻는 단어의 분산표현도 달라질 수 밖에 없다.\n",
    "\n",
    "여기서 우리가 다루는 모델은 클래스의 분류를 하는 신경망이다. 따라서, 학습하기 위해서는 Softmax와 Cross Entropy Error를 이용한다. 여기서는 스코어를 Softmax로 확률로 변환하고, 그 확률과 정답 라벨의 Cross Entropy Error를 구한다. 그리고 그 손실을 통해 학습을 진행한다.\n",
    "\n",
    "따라서 우리는 Softmax + Cross Entropy Error = Softmax with Loss를 구할 것이다.\n",
    "\n",
    "## Word2vec의 가중치와 분산표현\n",
    "word2vec에서 사용되는 신경망에는 2가지의 가중치가 존재한다. 하나는 입력층의 가중치(Win)과 출력층의 가중치(Wout)이다. 입력층의 가중치 Win의 각 행은 각 단어의 분산표현에 해당한다. 그리고 출력층의 가중치 Wout에도 단어의 의미가 인코딩된 벡터가 들어있다. 둘이 다른 건, 방향 뿐이다. \n",
    "\n",
    "그렇다면 이 두 가지 중에서 어떤 가중치를 쓸까? word2vec, 특히 skip-gram 모델에서는 입력층만을 이용한다. GloVe라는 방법은 두개의 가중치를 모두 이용한다.\n",
    "\n",
    "## Context와 Target\n",
    "자, 그럼 우리의 목표는 뭘까? Context가 주어질 때, Target이 출현하는 확률을 높이는 것이다. 그렇다면 코퍼스에서 Context와 Target을 우선 벡터로 따로 분리하는 작업을 진행해보자.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 1 2 3 4 5 6 1 2 3 7 8]\n{0: 'i', 1: 'want', 2: 'to', 3: 'study', 4: 'python', 5: 'and', 6: 'you', 7: 'javascript', 8: '.'}\n"
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "text = \"I want to study Python and you want to study JavaScript.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0 2]\n [1 3]\n [2 4]\n [3 5]\n [4 6]\n [5 1]\n [6 2]\n [1 3]\n [2 7]\n [3 8]]\n[1 2 3 4 5 6 1 2 3 7]\n"
    }
   ],
   "source": [
    "# context와 target을 출력해보자. corpus가 [0,1,2,3]이라면\n",
    "# context는 [[0, 2], [1,3]], target은 [1, 2]로 출력하는 함수를 만들자.\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size+1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "    \n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "print(contexts)\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 이 값들을 one-hot-vector로 바꿔서 CBOW 모델에 주자.\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "    \"\"\"\n",
    "    :param corpus: 단어ID 리스트(1 or 2차원의 numpy 배열)\n",
    "    :param vocab_size: 어휘수\n",
    "    :return: one-hot 배열(2 or 3차원의 numpy 배열)\n",
    "    \"\"\"\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "# 위의 예제에서 계속.\n",
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습 데이터의 준비는 끝났다. CBOW 모델을 실제 코드로 옮겨보자.\n",
    "먼저, SoftMaxWithLoss layer를 코드로 작성하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 정답 데이터가 one-hot-vector일 때, 정답 라벨의 인덱스로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "\n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None # softmax 출력\n",
    "        self.t = None # 정답라벨\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 정답 라벨이 one-hot 벡터일 때, 정답의 인덱스로 변환\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx /= batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " epoch 601 |  iter 1 / 2 | time 1[s] | loss 0.76\n| epoch 602 |  iter 1 / 2 | time 1[s] | loss 0.49\n| epoch 603 |  iter 1 / 2 | time 1[s] | loss 0.91\n| epoch 604 |  iter 1 / 2 | time 1[s] | loss 0.70\n| epoch 605 |  iter 1 / 2 | time 1[s] | loss 0.56\n| epoch 606 |  iter 1 / 2 | time 1[s] | loss 0.90\n| epoch 607 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 608 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 609 |  iter 1 / 2 | time 1[s] | loss 0.70\n| epoch 610 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 611 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 612 |  iter 1 / 2 | time 1[s] | loss 0.77\n| epoch 613 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 614 |  iter 1 / 2 | time 1[s] | loss 0.75\n| epoch 615 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 616 |  iter 1 / 2 | time 1[s] | loss 0.75\n| epoch 617 |  iter 1 / 2 | time 1[s] | loss 0.56\n| epoch 618 |  iter 1 / 2 | time 1[s] | loss 0.76\n| epoch 619 |  iter 1 / 2 | time 1[s] | loss 0.82\n| epoch 620 |  iter 1 / 2 | time 1[s] | loss 0.48\n| epoch 621 |  iter 1 / 2 | time 1[s] | loss 0.74\n| epoch 622 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 623 |  iter 1 / 2 | time 1[s] | loss 0.61\n| epoch 624 |  iter 1 / 2 | time 1[s] | loss 0.82\n| epoch 625 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 626 |  iter 1 / 2 | time 1[s] | loss 0.70\n| epoch 627 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 628 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 629 |  iter 1 / 2 | time 1[s] | loss 0.55\n| epoch 630 |  iter 1 / 2 | time 1[s] | loss 0.76\n| epoch 631 |  iter 1 / 2 | time 1[s] | loss 0.68\n| epoch 632 |  iter 1 / 2 | time 1[s] | loss 0.82\n| epoch 633 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 634 |  iter 1 / 2 | time 1[s] | loss 0.82\n| epoch 635 |  iter 1 / 2 | time 1[s] | loss 0.53\n| epoch 636 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 637 |  iter 1 / 2 | time 1[s] | loss 0.68\n| epoch 638 |  iter 1 / 2 | time 1[s] | loss 0.82\n| epoch 639 |  iter 1 / 2 | time 1[s] | loss 0.68\n| epoch 640 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 641 |  iter 1 / 2 | time 1[s] | loss 0.61\n| epoch 642 |  iter 1 / 2 | time 1[s] | loss 0.75\n| epoch 643 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 644 |  iter 1 / 2 | time 1[s] | loss 0.76\n| epoch 645 |  iter 1 / 2 | time 1[s] | loss 0.61\n| epoch 646 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 647 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 648 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 649 |  iter 1 / 2 | time 1[s] | loss 0.70\n| epoch 650 |  iter 1 / 2 | time 1[s] | loss 0.60\n| epoch 651 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 652 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 653 |  iter 1 / 2 | time 1[s] | loss 0.75\n| epoch 654 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 655 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 656 |  iter 1 / 2 | time 1[s] | loss 0.76\n| epoch 657 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 658 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 659 |  iter 1 / 2 | time 1[s] | loss 0.81\n| epoch 660 |  iter 1 / 2 | time 1[s] | loss 0.53\n| epoch 661 |  iter 1 / 2 | time 1[s] | loss 0.74\n| epoch 662 |  iter 1 / 2 | time 1[s] | loss 0.52\n| epoch 663 |  iter 1 / 2 | time 1[s] | loss 0.68\n| epoch 664 |  iter 1 / 2 | time 1[s] | loss 0.73\n| epoch 665 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 666 |  iter 1 / 2 | time 1[s] | loss 0.82\n| epoch 667 |  iter 1 / 2 | time 1[s] | loss 0.52\n| epoch 668 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 669 |  iter 1 / 2 | time 1[s] | loss 0.74\n| epoch 670 |  iter 1 / 2 | time 1[s] | loss 0.68\n| epoch 671 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 672 |  iter 1 / 2 | time 1[s] | loss 0.74\n| epoch 673 |  iter 1 / 2 | time 1[s] | loss 0.51\n| epoch 674 |  iter 1 / 2 | time 1[s] | loss 0.74\n| epoch 675 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 676 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 677 |  iter 1 / 2 | time 1[s] | loss 0.74\n| epoch 678 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 679 |  iter 1 / 2 | time 1[s] | loss 0.68\n| epoch 680 |  iter 1 / 2 | time 1[s] | loss 0.65\n| epoch 681 |  iter 1 / 2 | time 1[s] | loss 0.80\n| epoch 682 |  iter 1 / 2 | time 1[s] | loss 0.61\n| epoch 683 |  iter 1 / 2 | time 1[s] | loss 0.57\n| epoch 684 |  iter 1 / 2 | time 1[s] | loss 0.81\n| epoch 685 |  iter 1 / 2 | time 1[s] | loss 0.53\n| epoch 686 |  iter 1 / 2 | time 1[s] | loss 0.64\n| epoch 687 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 688 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 689 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 690 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 691 |  iter 1 / 2 | time 1[s] | loss 0.73\n| epoch 692 |  iter 1 / 2 | time 1[s] | loss 0.72\n| epoch 693 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 694 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 695 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 696 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 697 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 698 |  iter 1 / 2 | time 1[s] | loss 0.73\n| epoch 699 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 700 |  iter 1 / 2 | time 1[s] | loss 0.64\n| epoch 701 |  iter 1 / 2 | time 1[s] | loss 0.72\n| epoch 702 |  iter 1 / 2 | time 1[s] | loss 0.73\n| epoch 703 |  iter 1 / 2 | time 1[s] | loss 0.54\n| epoch 704 |  iter 1 / 2 | time 1[s] | loss 0.86\n| epoch 705 |  iter 1 / 2 | time 1[s] | loss 0.45\n| epoch 706 |  iter 1 / 2 | time 1[s] | loss 0.72\n| epoch 707 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 708 |  iter 1 / 2 | time 1[s] | loss 0.79\n| epoch 709 |  iter 1 / 2 | time 1[s] | loss 0.50\n| epoch 710 |  iter 1 / 2 | time 1[s] | loss 0.80\n| epoch 711 |  iter 1 / 2 | time 1[s] | loss 0.50\n| epoch 712 |  iter 1 / 2 | time 1[s] | loss 0.88\n| epoch 713 |  iter 1 / 2 | time 1[s] | loss 0.43\n| epoch 714 |  iter 1 / 2 | time 1[s] | loss 0.74\n| epoch 715 |  iter 1 / 2 | time 1[s] | loss 0.71\n| epoch 716 |  iter 1 / 2 | time 1[s] | loss 0.51\n| epoch 717 |  iter 1 / 2 | time 1[s] | loss 0.72\n| epoch 718 |  iter 1 / 2 | time 1[s] | loss 0.64\n| epoch 719 |  iter 1 / 2 | time 1[s] | loss 0.56\n| epoch 720 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 721 |  iter 1 / 2 | time 1[s] | loss 0.78\n| epoch 722 |  iter 1 / 2 | time 1[s] | loss 0.56\n| epoch 723 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 724 |  iter 1 / 2 | time 1[s] | loss 0.65\n| epoch 725 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 726 |  iter 1 / 2 | time 1[s] | loss 0.57\n| epoch 727 |  iter 1 / 2 | time 1[s] | loss 0.74\n| epoch 728 |  iter 1 / 2 | time 1[s] | loss 0.64\n| epoch 729 |  iter 1 / 2 | time 1[s] | loss 0.49\n| epoch 730 |  iter 1 / 2 | time 1[s] | loss 0.87\n| epoch 731 |  iter 1 / 2 | time 1[s] | loss 0.42\n| epoch 732 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 733 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 734 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 735 |  iter 1 / 2 | time 1[s] | loss 0.78\n| epoch 736 |  iter 1 / 2 | time 1[s] | loss 0.57\n| epoch 737 |  iter 1 / 2 | time 1[s] | loss 0.80\n| epoch 738 |  iter 1 / 2 | time 1[s] | loss 0.57\n| epoch 739 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 740 |  iter 1 / 2 | time 1[s] | loss 0.51\n| epoch 741 |  iter 1 / 2 | time 1[s] | loss 0.80\n| epoch 742 |  iter 1 / 2 | time 1[s] | loss 0.50\n| epoch 743 |  iter 1 / 2 | time 1[s] | loss 0.71\n| epoch 744 |  iter 1 / 2 | time 1[s] | loss 0.64\n| epoch 745 |  iter 1 / 2 | time 1[s] | loss 0.55\n| epoch 746 |  iter 1 / 2 | time 1[s] | loss 0.73\n| epoch 747 |  iter 1 / 2 | time 1[s] | loss 0.71\n| epoch 748 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 749 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 750 |  iter 1 / 2 | time 1[s] | loss 0.48\n| epoch 751 |  iter 1 / 2 | time 1[s] | loss 0.77\n| epoch 752 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 753 |  iter 1 / 2 | time 1[s] | loss 0.57\n| epoch 754 |  iter 1 / 2 | time 1[s] | loss 0.68\n| epoch 755 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 756 |  iter 1 / 2 | time 1[s] | loss 0.77\n| epoch 757 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 758 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 759 |  iter 1 / 2 | time 1[s] | loss 0.71\n| epoch 760 |  iter 1 / 2 | time 1[s] | loss 0.56\n| epoch 761 |  iter 1 / 2 | time 1[s] | loss 0.47\n| epoch 762 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 763 |  iter 1 / 2 | time 1[s] | loss 0.61\n| epoch 764 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 765 |  iter 1 / 2 | time 1[s] | loss 0.72\n| epoch 766 |  iter 1 / 2 | time 1[s] | loss 0.70\n| epoch 767 |  iter 1 / 2 | time 1[s] | loss 0.70\n| epoch 768 |  iter 1 / 2 | time 1[s] | loss 0.43\n| epoch 769 |  iter 1 / 2 | time 1[s] | loss 0.68\n| epoch 770 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 771 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 772 |  iter 1 / 2 | time 1[s] | loss 0.61\n| epoch 773 |  iter 1 / 2 | time 1[s] | loss 0.70\n| epoch 774 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 775 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 776 |  iter 1 / 2 | time 1[s] | loss 0.49\n| epoch 777 |  iter 1 / 2 | time 1[s] | loss 0.72\n| epoch 778 |  iter 1 / 2 | time 1[s] | loss 0.70\n| epoch 779 |  iter 1 / 2 | time 1[s] | loss 0.49\n| epoch 780 |  iter 1 / 2 | time 1[s] | loss 0.77\n| epoch 781 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 782 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 783 |  iter 1 / 2 | time 1[s] | loss 0.70\n| epoch 784 |  iter 1 / 2 | time 1[s] | loss 0.53\n| epoch 785 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 786 |  iter 1 / 2 | time 1[s] | loss 0.65\n| epoch 787 |  iter 1 / 2 | time 1[s] | loss 0.53\n| epoch 788 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 789 |  iter 1 / 2 | time 1[s] | loss 0.83\n| epoch 790 |  iter 1 / 2 | time 1[s] | loss 0.46\n| epoch 791 |  iter 1 / 2 | time 1[s] | loss 0.56\n| epoch 792 |  iter 1 / 2 | time 1[s] | loss 0.72\n| epoch 793 |  iter 1 / 2 | time 1[s] | loss 0.56\n| epoch 794 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 795 |  iter 1 / 2 | time 1[s] | loss 0.65\n| epoch 796 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 797 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 798 |  iter 1 / 2 | time 1[s] | loss 0.56\n| epoch 799 |  iter 1 / 2 | time 1[s] | loss 0.55\n| epoch 800 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 801 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 802 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 803 |  iter 1 / 2 | time 1[s] | loss 0.76\n| epoch 804 |  iter 1 / 2 | time 1[s] | loss 0.51\n| epoch 805 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 806 |  iter 1 / 2 | time 1[s] | loss 0.45\n| epoch 807 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 808 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 809 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 810 |  iter 1 / 2 | time 1[s] | loss 0.72\n| epoch 811 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 812 |  iter 1 / 2 | time 1[s] | loss 0.38\n| epoch 813 |  iter 1 / 2 | time 1[s] | loss 0.72\n| epoch 814 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 815 |  iter 1 / 2 | time 1[s] | loss 0.65\n| epoch 816 |  iter 1 / 2 | time 1[s] | loss 0.55\n| epoch 817 |  iter 1 / 2 | time 1[s] | loss 0.76\n| epoch 818 |  iter 1 / 2 | time 1[s] | loss 0.38\n| epoch 819 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 820 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 821 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 822 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 823 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 824 |  iter 1 / 2 | time 1[s] | loss 0.71\n| epoch 825 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 826 |  iter 1 / 2 | time 1[s] | loss 0.38\n| epoch 827 |  iter 1 / 2 | time 1[s] | loss 0.78\n| epoch 828 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 829 |  iter 1 / 2 | time 1[s] | loss 0.61\n| epoch 830 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 831 |  iter 1 / 2 | time 1[s] | loss 0.55\n| epoch 832 |  iter 1 / 2 | time 1[s] | loss 0.71\n| epoch 833 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 834 |  iter 1 / 2 | time 1[s] | loss 0.57\n| epoch 835 |  iter 1 / 2 | time 1[s] | loss 0.65\n| epoch 836 |  iter 1 / 2 | time 1[s] | loss 0.68\n| epoch 837 |  iter 1 / 2 | time 1[s] | loss 0.47\n| epoch 838 |  iter 1 / 2 | time 1[s] | loss 0.71\n| epoch 839 |  iter 1 / 2 | time 1[s] | loss 0.54\n| epoch 840 |  iter 1 / 2 | time 1[s] | loss 0.75\n| epoch 841 |  iter 1 / 2 | time 1[s] | loss 0.54\n| epoch 842 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 843 |  iter 1 / 2 | time 1[s] | loss 0.57\n| epoch 844 |  iter 1 / 2 | time 1[s] | loss 0.68\n| epoch 845 |  iter 1 / 2 | time 1[s] | loss 0.51\n| epoch 846 |  iter 1 / 2 | time 1[s] | loss 0.78\n| epoch 847 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 848 |  iter 1 / 2 | time 1[s] | loss 0.50\n| epoch 849 |  iter 1 / 2 | time 1[s] | loss 0.61\n| epoch 850 |  iter 1 / 2 | time 1[s] | loss 0.82\n| epoch 851 |  iter 1 / 2 | time 1[s] | loss 0.37\n| epoch 852 |  iter 1 / 2 | time 1[s] | loss 0.78\n| epoch 853 |  iter 1 / 2 | time 1[s] | loss 0.61\n| epoch 854 |  iter 1 / 2 | time 1[s] | loss 0.54\n| epoch 855 |  iter 1 / 2 | time 1[s] | loss 0.54\n| epoch 856 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 857 |  iter 1 / 2 | time 1[s] | loss 0.61\n| epoch 858 |  iter 1 / 2 | time 1[s] | loss 0.61\n| epoch 859 |  iter 1 / 2 | time 1[s] | loss 0.85\n| epoch 860 |  iter 1 / 2 | time 1[s] | loss 0.40\n| epoch 861 |  iter 1 / 2 | time 1[s] | loss 0.68\n| epoch 862 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 863 |  iter 1 / 2 | time 1[s] | loss 0.71\n| epoch 864 |  iter 1 / 2 | time 1[s] | loss 0.51\n| epoch 865 |  iter 1 / 2 | time 1[s] | loss 0.61\n| epoch 866 |  iter 1 / 2 | time 1[s] | loss 0.71\n| epoch 867 |  iter 1 / 2 | time 1[s] | loss 0.44\n| epoch 868 |  iter 1 / 2 | time 1[s] | loss 0.68\n| epoch 869 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 870 |  iter 1 / 2 | time 1[s] | loss 0.57\n| epoch 871 |  iter 1 / 2 | time 1[s] | loss 0.50\n| epoch 872 |  iter 1 / 2 | time 1[s] | loss 0.78\n| epoch 873 |  iter 1 / 2 | time 1[s] | loss 0.43\n| epoch 874 |  iter 1 / 2 | time 1[s] | loss 0.60\n| epoch 875 |  iter 1 / 2 | time 1[s] | loss 0.64\n| epoch 876 |  iter 1 / 2 | time 1[s] | loss 0.57\n| epoch 877 |  iter 1 / 2 | time 1[s] | loss 0.78\n| epoch 878 |  iter 1 / 2 | time 1[s] | loss 0.36\n| epoch 879 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 880 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 881 |  iter 1 / 2 | time 1[s] | loss 0.64\n| epoch 882 |  iter 1 / 2 | time 1[s] | loss 0.60\n| epoch 883 |  iter 1 / 2 | time 1[s] | loss 0.60\n| epoch 884 |  iter 1 / 2 | time 1[s] | loss 0.53\n| epoch 885 |  iter 1 / 2 | time 1[s] | loss 0.57\n| epoch 886 |  iter 1 / 2 | time 1[s] | loss 0.70\n| epoch 887 |  iter 1 / 2 | time 1[s] | loss 0.64\n| epoch 888 |  iter 1 / 2 | time 1[s] | loss 0.53\n| epoch 889 |  iter 1 / 2 | time 1[s] | loss 0.77\n| epoch 890 |  iter 1 / 2 | time 1[s] | loss 0.50\n| epoch 891 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 892 |  iter 1 / 2 | time 1[s] | loss 0.46\n| epoch 893 |  iter 1 / 2 | time 1[s] | loss 0.64\n| epoch 894 |  iter 1 / 2 | time 1[s] | loss 0.56\n| epoch 895 |  iter 1 / 2 | time 1[s] | loss 0.60\n| epoch 896 |  iter 1 / 2 | time 1[s] | loss 0.50\n| epoch 897 |  iter 1 / 2 | time 1[s] | loss 0.84\n| epoch 898 |  iter 1 / 2 | time 1[s] | loss 0.50\n| epoch 899 |  iter 1 / 2 | time 1[s] | loss 0.70\n| epoch 900 |  iter 1 / 2 | time 1[s] | loss 0.50\n| epoch 901 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 902 |  iter 1 / 2 | time 1[s] | loss 0.57\n| epoch 903 |  iter 1 / 2 | time 1[s] | loss 0.56\n| epoch 904 |  iter 1 / 2 | time 1[s] | loss 0.60\n| epoch 905 |  iter 1 / 2 | time 1[s] | loss 0.74\n| epoch 906 |  iter 1 / 2 | time 1[s] | loss 0.53\n| epoch 907 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 908 |  iter 1 / 2 | time 1[s] | loss 0.60\n| epoch 909 |  iter 1 / 2 | time 1[s] | loss 0.60\n| epoch 910 |  iter 1 / 2 | time 1[s] | loss 0.46\n| epoch 911 |  iter 1 / 2 | time 1[s] | loss 0.56\n| epoch 912 |  iter 1 / 2 | time 1[s] | loss 0.60\n| epoch 913 |  iter 1 / 2 | time 1[s] | loss 0.67\n| epoch 914 |  iter 1 / 2 | time 1[s] | loss 0.70\n| epoch 915 |  iter 1 / 2 | time 1[s] | loss 0.49\n| epoch 916 |  iter 1 / 2 | time 1[s] | loss 0.70\n| epoch 917 |  iter 1 / 2 | time 1[s] | loss 0.53\n| epoch 918 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 919 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 920 |  iter 1 / 2 | time 1[s] | loss 0.35\n| epoch 921 |  iter 1 / 2 | time 1[s] | loss 0.81\n| epoch 922 |  iter 1 / 2 | time 1[s] | loss 0.49\n| epoch 923 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 924 |  iter 1 / 2 | time 1[s] | loss 0.70\n| epoch 925 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 926 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 927 |  iter 1 / 2 | time 1[s] | loss 0.49\n| epoch 928 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 929 |  iter 1 / 2 | time 1[s] | loss 0.56\n| epoch 930 |  iter 1 / 2 | time 1[s] | loss 0.77\n| epoch 931 |  iter 1 / 2 | time 1[s] | loss 0.35\n| epoch 932 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 933 |  iter 1 / 2 | time 1[s] | loss 0.56\n| epoch 934 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 935 |  iter 1 / 2 | time 1[s] | loss 0.52\n| epoch 936 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 937 |  iter 1 / 2 | time 1[s] | loss 0.63\n| epoch 938 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 939 |  iter 1 / 2 | time 1[s] | loss 0.48\n| epoch 940 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 941 |  iter 1 / 2 | time 1[s] | loss 0.77\n| epoch 942 |  iter 1 / 2 | time 1[s] | loss 0.52\n| epoch 943 |  iter 1 / 2 | time 1[s] | loss 0.73\n| epoch 944 |  iter 1 / 2 | time 1[s] | loss 0.38\n| epoch 945 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 946 |  iter 1 / 2 | time 1[s] | loss 0.52\n| epoch 947 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 948 |  iter 1 / 2 | time 1[s] | loss 0.52\n| epoch 949 |  iter 1 / 2 | time 1[s] | loss 0.73\n| epoch 950 |  iter 1 / 2 | time 1[s] | loss 0.48\n| epoch 951 |  iter 1 / 2 | time 1[s] | loss 0.52\n| epoch 952 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 953 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 954 |  iter 1 / 2 | time 1[s] | loss 0.59\n| epoch 955 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 956 |  iter 1 / 2 | time 1[s] | loss 0.41\n| epoch 957 |  iter 1 / 2 | time 1[s] | loss 0.73\n| epoch 958 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 959 |  iter 1 / 2 | time 1[s] | loss 0.41\n| epoch 960 |  iter 1 / 2 | time 1[s] | loss 0.76\n| epoch 961 |  iter 1 / 2 | time 1[s] | loss 0.41\n| epoch 962 |  iter 1 / 2 | time 1[s] | loss 0.76\n| epoch 963 |  iter 1 / 2 | time 1[s] | loss 0.51\n| epoch 964 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 965 |  iter 1 / 2 | time 1[s] | loss 0.44\n| epoch 966 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 967 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 968 |  iter 1 / 2 | time 1[s] | loss 0.41\n| epoch 969 |  iter 1 / 2 | time 1[s] | loss 0.73\n| epoch 970 |  iter 1 / 2 | time 1[s] | loss 0.44\n| epoch 971 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 972 |  iter 1 / 2 | time 1[s] | loss 0.51\n| epoch 973 |  iter 1 / 2 | time 1[s] | loss 0.76\n| epoch 974 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 975 |  iter 1 / 2 | time 1[s] | loss 0.51\n| epoch 976 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 977 |  iter 1 / 2 | time 1[s] | loss 0.48\n| epoch 978 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 979 |  iter 1 / 2 | time 1[s] | loss 0.51\n| epoch 980 |  iter 1 / 2 | time 1[s] | loss 0.66\n| epoch 981 |  iter 1 / 2 | time 1[s] | loss 0.47\n| epoch 982 |  iter 1 / 2 | time 1[s] | loss 0.76\n| epoch 983 |  iter 1 / 2 | time 1[s] | loss 0.51\n| epoch 984 |  iter 1 / 2 | time 1[s] | loss 0.51\n| epoch 985 |  iter 1 / 2 | time 1[s] | loss 0.73\n| epoch 986 |  iter 1 / 2 | time 1[s] | loss 0.48\n| epoch 987 |  iter 1 / 2 | time 1[s] | loss 0.69\n| epoch 988 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 989 |  iter 1 / 2 | time 1[s] | loss 0.55\n| epoch 990 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 991 |  iter 1 / 2 | time 1[s] | loss 0.44\n| epoch 992 |  iter 1 / 2 | time 1[s] | loss 0.76\n| epoch 993 |  iter 1 / 2 | time 1[s] | loss 0.47\n| epoch 994 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 995 |  iter 1 / 2 | time 1[s] | loss 0.62\n| epoch 996 |  iter 1 / 2 | time 1[s] | loss 0.40\n| epoch 997 |  iter 1 / 2 | time 1[s] | loss 0.65\n| epoch 998 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 999 |  iter 1 / 2 | time 1[s] | loss 0.58\n| epoch 1000 |  iter 1 / 2 | time 1[s] | loss 0.58\n"
    }
   ],
   "source": [
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # parameter의 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f') # float32형으로\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # 레이어 생성\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # 모든 가중치와 grad를 리스트에 모음\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "        # 멤버 변수에 단어의 분산표현을 설정\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = (h0 + h1) / 2\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer0.backward(da)\n",
    "        self.in_layer1.backward(da)\n",
    "        return None\n",
    "\n",
    "# 이제 본격적으로 학습해보자.\n",
    "import os\n",
    "module_path = os.path.abspath(\".\")\n",
    "from common.trainer import Trainer\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "text = \"I study Python and You study JavaScript.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "i [-1.0901577  1.1491778  1.078492  -1.1051644 -1.2962   ]\nstudy [ 1.154049   -1.1632253  -1.186218    1.1233807  -0.01793403]\npython [-0.7974376   0.6964599   0.7716731  -0.78829056 -0.68517363]\nand [ 0.89068294 -0.8980191  -0.85832137  0.86041087 -2.037612  ]\nyou [-0.779569    0.70580465  0.77850765 -0.7886558  -0.7089577 ]\njavascript [-1.0965396  1.1460419  1.0555729 -1.101946  -1.2720102]\n. [ 1.0861663 -1.1255072 -1.1605011  1.1106279  1.6983181]\n"
    }
   ],
   "source": [
    "word_vecs = model.word_vecs\n",
    "\n",
    "# 각 단어를 벡터로 표현했다!! 이것이 단어의 분산표현이다.\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW 모델과 확률\n",
    "CBOW는 주변 단어들로 가운데 target을 추측하는 방법이다. 이를 확률로 나타내면 다음과 같다.\n",
    "- P(wt | wt-1, wt+1)\n",
    "\n",
    "이는, wt-1과 wt+1이 일어난 후에 wt가 일어날 확률을 뜻한다. 여기에 Cross Entropy Error를 적용하면, 다음과 같은 식이 된다.\n",
    "- L = -logP(wt | wt-1, wt+1)\n",
    "\n",
    "단순히 확률에 대해 로그를 취하고 마이너스를 더한 값이다. 이는 negative log likelihood라고 불린다! 이 식은 한 개의 샘플 데이터에 대한 손실함수이므로, 이를 코퍼스 전체로 확장하면 아래와 같은 식이 된다.\n",
    "- L = -(1/T)시그마(t=1 ~ T)logP(wt | wt-1, wt+1)\n",
    "\n",
    "CBOW 모델으로 학습을 진행한다는 것은, 위 L값을 최대한 작게한다는 것이다.\n",
    "\n",
    "## Skip-gram\n",
    "skip-gram은 가운데 단어가 주어졌을 때, 주변의 단어를 추측하는 방법이다. CBOW와 반대지. 그래서 신경망의 구조도 반대가 된다.\n",
    "\n",
    "입력층이 한개고, 출력층이 문맥의 수만큼 존재한다. 그렇다면 이를 확률로 나타낸다면 어떻게 될까? \n",
    "- P(wt-1, wt+1 | wt)\n",
    "\n",
    "가 될 것이다. skip-gram모델에서는 문맥의 단어 사이에 관련성이 없다고 판단하기 때문에, 이를 다음과 같이 풀 수 있다.\n",
    "- P(wt-1 | wt)P(wt+1 | wt)\n",
    "\n",
    "여기에 cross entropy error를 적용하면, \n",
    "- L = -(1/T)시그마(t=1 ~ T)(P(wt-1 | wt) + P(wt+1 | wt))이 된다.\n",
    "\n",
    "그렇다면, CBOW와 skip-gram 중 뭘 써야 할까? 답은 skip-gram이다. 코퍼스가 커지면 커질수록 저빈도의 단어나 유추문제 성능 면에서 더 우수한 게 증명되었기 때문이다. 다만 학습속도에서는 CBOW가 더 우수하다. 왜냐? skip-gram 모델은 문맥 수만큼 손실을 구해야 하기 때문에, 계산이 그만큼 복잡해지기 때문이다(위의 L을 비교하면 알 수 있음)."
   ]
  }
 ]
}