{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37164bitbasecondac1ab1b3ac90d4e2981ba357c16f579f5",
   "display_name": "Python 3.7.1 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어와 단어의 분산표현\n",
    "\n",
    "이 파일에서는 컴퓨터에 언어를 이해시키는 것이 어떤 것인지, 그리고 어떤 접근법이 존재하는지를 중심으로 고찰해본다. 특히 딥러닝 등장 이전의 방법에 대해 자세히 잘펴본다. 다음 ch3 파일부터는 딥러닝을 기반으로 한 방법론을 살펴볼 것이다.\n",
    "\n",
    "또한, 이 파일에서는 문장을 파이썬으로 다루는 연습을 한다. 문장을 단어로 분할하는 처리나, 단어를 단어 ID로 변환하는 처리 등을 실습한다.\n",
    "\n",
    "자, 자연어처리의 세계로!\n",
    "\n",
    "## 단어의 의미\n",
    "자연어에서 '단어'란 최소한의 의미 단위이다. 따라서 자연어를 컴퓨터에 이해시키기 위해서는, 단어의 의미를 이해시키는 것이 가장 중요하다고도 할 수 있다. \n",
    "\n",
    "그렇다면 단어의 의미를 잘 캐치하려면 어떻게 해야할까? 구체적으로는 ch2와 ch3에서 다음 3가지 방법을 살펴볼 것이다.\n",
    "- Thesarus에 의한 방법\n",
    "- 카운트 기반의 방법\n",
    "- 추론 기반의 방법 (word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Thesarus(단어 사전)\n",
    "'단어의 의미'를 표현하기 위해선, 사람의 손으로 단어의 의미를 정의하는 방법을 생각할 수 있다. 이 방법 중 하나가 바로 사전과 같이 단어 하나하나에 대해 의미를 정의하는 것이다. \n",
    "\n",
    "자연어처리의 역사를 돌아보면 단어의 의미를 사람의 손으로 정의하려는 시도가 많이 있었다. 그러나 사람이 쓰는 '옥스포드 사전'같은 사전이 아닌, 'Thesarus'라고 불리는 사전이 많이 사용되었다. 그렇다면 Thesarus란?\n",
    "\n",
    "- Thesarus는 유의어 사전으로, 동의어와 유의어가 같은 그룹에 분류된다.(car = automobile, machine, motocar)\n",
    "- Thesarus는 그래프 구조로 각 단어 간의 상하관계를 정의한다.\n",
    "\n",
    "이렇게 Thesaurs는 모든 단어에 대해 유의어의 조합을 만들고 관계를 그래프로 표현하여, 단어간의 관계를 표시한다. 대표적인 예로는 WordNet이 있다. WordNet을 사용하면 단어 간의 유사도를 측정할 수 있다.\n",
    "\n",
    "### 문제점\n",
    "- 사람의 손으로 일일이 라벨을 붙여줘야 한다.\n",
    "- 신조어 등, 시대의 변화에 대응하는 것이 힘들다.\n",
    "- 인건비가 비싸다.\n",
    "- 단어의 사소한 뉘앙스 차이를 표현하지 못한다.\n",
    "\n",
    "이러한 문제를 해결하기 위해, 코퍼스(corpus)를 이용한 count-based 방식이 도입되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Synset('car.n.01'),\n Synset('car.n.02'),\n Synset('car.n.03'),\n Synset('car.n.04'),\n Synset('cable_car.n.01')]"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# WordNet을 Python에서 이용하기 위해서는, NLTK라는 라이브러리를 사용\n",
    "# 품사태그와 구문분석, 정보추출과 의미해석 등의 기능이 있음\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# car라는 단어에 얼마나 다른 의미가 존재하는지 확인해보자.\n",
    "# 5개의 다른 동의어 그룹이 정의되어 있는 것을 확인 가능하다.\n",
    "wordnet.synsets('car')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 car.n.01이라는 것은, 'car라는 명사의 첫번째'라는 의미가 있다는 것을 나타낸다. 이렇게 표시하는 이유는 여러가지 의미 중 의미를 특정하기 위해 사용하는 것이다.\n",
    "\n",
    "그렇다면 car.n.01의 의미를 알아보고, 동의어로 어떤 단어가 존재하는지 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "car = wordnet.synset('car.n.01') #동의어 그룹\n",
    "car.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['car', 'auto', 'automobile', 'machine', 'motorcar']"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "car.lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "car와 다른 단어의 상위, 하위 관계를 알아보자. hypernym_path()라는 메소드를 쓴다. hypernym은 '상위어'라는 의미가 있다. \n",
    "\n",
    "아래 결과에서 car의 상위어가 motor_vehicle, self-propelled_vehicle, ... 인 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Synset('entity.n.01'),\n Synset('physical_entity.n.01'),\n Synset('object.n.01'),\n Synset('whole.n.02'),\n Synset('artifact.n.01'),\n Synset('instrumentality.n.03'),\n Synset('container.n.01'),\n Synset('wheeled_vehicle.n.01'),\n Synset('self-propelled_vehicle.n.01'),\n Synset('motor_vehicle.n.01'),\n Synset('car.n.01')]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "car.hypernym_paths()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어간의 유의도를 산출하기 위해서는 path_similarity()라는 메소드를 사용한다. 이 메소드가 반환하는 값은 0부터 1 범위의 실수이다. 값이 클수록 비슷한 단어이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.05555555555555555\n0.07692307692307693\n0.3333333333333333\n"
    }
   ],
   "source": [
    "car = wordnet.synset('car.n.01')\n",
    "novel = wordnet.synset('novel.n.01')\n",
    "dog = wordnet.synset('dog.n.01')\n",
    "motorcycle = wordnet.synset('motorcycle.n.01')\n",
    "\n",
    "print(car.path_similarity(novel))\n",
    "print(car.path_similarity(dog))\n",
    "print(car.path_similarity(motorcycle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 카운트 기반 방법\n",
    "이 방법에서는 코퍼스를 사용한다. 코퍼스는 대량의 텍스트 데이터이다. 이것저것 모은 것이 아닌, 자연어처리의 연구와 어플리케이션을 위한 목적으로 수집된 데이터를 일반적으로 '코퍼스'라고 한다.\n",
    "\n",
    "이 방법의 목적은 사람의 지식이 들어있는 코퍼스로부터 자동으로 그 정수를 추출하는 것이다.\n",
    "\n",
    "코퍼스에는 다양한 종류가 있다. 위키피디아나 나츠메 소세키의 소설 등, 다양한 텍스트가 코퍼스로 사용된다. 여기서는 먼저 한 문장으로 구성된 단순한 텍스트를 코퍼스로서 사용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'i study nlp and you study css .'"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "text = 'I study NLP and you study CSS.'\n",
    "text = text.lower()\n",
    "text = text.replace('.', ' .')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['i', 'study', 'nlp', 'and', 'you', 'study', 'css', '.']"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "words = text.split(' ')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어의 ID와 단어의 대응표를 파이썬 딕셔너리로 만든다.\n",
    "# 이 딕셔너리를 사용하면 단어에서 id를 검색하거나, id에서 단어를 검색할 수 있다.\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WTI:  {'i': 0, 'study': 1, 'nlp': 2, 'and': 3, 'you': 4, 'css': 5, '.': 6}\nITW:  {0: 'i', 1: 'study', 2: 'nlp', 3: 'and', 4: 'you', 5: 'css', 6: '.'}\n"
    }
   ],
   "source": [
    "print('WTI: ', word_to_id)\n",
    "print('ITW: ',id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 1, 2, 3, 4, 1, 5, 6])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# 단어의 리스트를 단어id의 리스트로 변환해보자.\n",
    "import numpy as np\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 과정을 preprocess()라는 함수로, 정리하자.\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "text = 'You study Web and I study Javascript.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이로서 코퍼스의 전처리는 끝났다. \n",
    "- corpus는 단어ID의 리스트\n",
    "- word_to_id는 단어->단어ID의 딕셔너리\n",
    "- id_to_word는 그 역\n",
    "\n",
    "이제부터의 목표는 코퍼스를 써서 '단어의 의미'를 추출하는 것이다. '카운트 기반 방법'을 쓰면 단어를 벡터로 표시할 수 있게 된다.\n",
    "\n",
    "### 단어의 분산표현\n",
    "'단어의 의미'를 정확히 나타내는 벡터 표현을 찾아야 한다. 자연어처리 분야에서, 이는 단어의 '분산표현'이라고 불린다.\n",
    "\n",
    "## 분배가설\n",
    "단어를 벡터로 표현하기 위해, 많은 연구가 이루어져 왔다. 이 연구는 대부분 '단어의 의미는 주변 단어에 의해 형성된다'는 가설(=분배가설, distributional hypothesis)에 기반하고 있다. 단어 자체에는 의미가 없고, 그 단어가 사용되는 문맥에 의해 의미가 형성된다는 것이다.\n",
    "\n",
    "또한 문맥의 사이즈, 즉 주위 단어를 얼마나 포함하고 있는가를 'window size'라고 한다. window size가 1이면 좌우의 1단어가 되는 것이다.\n",
    "\n",
    "그렇다면 분배가설에 기반해 단어를 벡터로 표현하는 방법을 생각해보자. 특정 단어 주변에 어떤 단어가 얼마나 나타나는지를 세어, 집계하는 방법이 있을 것이다 이 방법을 '카운트 기반 방법' 또는 '통계적 방법'이라고 한다.\n",
    "\n",
    "## Co-occurence Matrix\n",
    "'you say goodbye and I say hello.'라는 문장이 있다고 하자. window size가 1이라면, you의 문맥에 포함되는 단어 빈도는 `[0, 1, 0, 0, 0, 0, 0, 0(. 포함)]`이다. 마찬가지로 say라면 `[1, 0, 1, 0, 1, 0, 1, 0]`이다. 이 과정을 모든 단어에 대해 수행하면, Co-occurence 행렬이라는 행렬을 만들 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 1 0 0 0 0 0]\n[0 1 0 1 0 0 0]\n[0 1 0 1 0 0 0]\n"
    }
   ],
   "source": [
    "text = 'you say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "C = np.array([\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0, 0 ,0],\n",
    "    [0, 0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0]\n",
    "], dtype=np.int32)\n",
    "\n",
    "print(C[0])\n",
    "print(C[4])\n",
    "print(C[word_to_id['goodbye']]) # [goodbye]의 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - 1\n",
    "            right_idx = idx + 1\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "            \n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "    \n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cocurrence matrix를 구했으니, 이제 벡터간의 유사도를 코사인 유사도로 측정해보자. 코사인 유사도는 벡터를 정규화하고, 내적을 곱하는 방법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps를 분모에 더한 것은, \n",
    "# 원소가 모두 0인 벡터로 인한 divide by zero 에러를 막기 위해.\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / np.sqrt(np.sum(x**2) + eps) # x 정규화\n",
    "    ny = y / np.sqrt(np.sum(y**2) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.7071067811865475\n"
    }
   ],
   "source": [
    "# 그렇다면, 이제 you 와 i 의 유사도를 알아보자.\n",
    "text = 'you say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['you']] # you의 단어 벡터\n",
    "c1 = C[word_to_id['i']] # i의 단어 벡터\n",
    "\n",
    "# 코사인 유사도는 -1 ~ 1의 값을 갖기 때문에, 0.7은 꽤 높은 수치이다.\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 단어가 쿼리로 주어졌을 때, 비슷한 단어를 내림차순으로 표시해주는 함수를 짜보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    # 쿼리를 꺼낸다.\n",
    "    if query not in word_to_id:\n",
    "        print(f'{query} is not found')\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    # 쿼리 단어의 벡터와, 다른 모든 단어 벡터와의 코사인 유사도를 각각 구한다.\n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # 코사인 유사도의 결과로부터, 그 값을 내림차순으로 출력\n",
    "    count = 0\n",
    "    # similarity 배열 속 원소의 인덱스를 높은 값부터 정렬\n",
    "    # argsort()는 배열 원소를 오름차순으로 정렬하여, 인덱스를 반환.\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(f'{id_to_word[i]}: {similarity[i]:.5f}')\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n[query] i\ngoodbye: 1.00000\nyou: 0.70711\nhello: 0.50000\nsay: 0.00000\nand: 0.00000\n"
    }
   ],
   "source": [
    "most_similar('i', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 살펴본 바로는 i와 가장 유사한 단어는 you가 아닌, goodbye라고 나온다. 상식과 어긋나는 결과이다. 이는 코퍼스 크기가 작은 것도 있지만, 아직 개선해야 할 점이 남았기 때문이기도 하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카운트 기반 방법의 개선\n",
    "위의 Cocurrence 행렬의 요소는, 두 개의 단어가 같이 나타난 횟수를 나타낸다. 그러나, 이는 'the'와 같은 단어가 높은 연관성이 나타나는 문제점이 있다. 'the car'를 예로 들어보자면, 'the'와 'car'는 바로 붙여서 쓰는 경우가 많다. 따라서 더 유사성이 높아야 하는 'drive'보다 'the'가 'car'와 더 높은 유사성을 보이게 되는 문제가 발생한다.\n",
    "\n",
    "이러한 문제점을 해결하기 위해, 상호정보량(PMI, Pointwise Mutual Information)이라고 불리는 지표가 쓰인다. 이는 x와 y라는 확률변수에 의해 정의된다. PMI는, 그 값이 높을수록 관련성이 높다는 것을 의미한다. PMI는 또한 Cocurrence matrix로부터 구할 수 있다. PMI를 사용하면, the와 car의 연관성보다 car와 drive의 연관성이 높게 나오는 것을 알 수 있다.\n",
    "\n",
    "PMI도 문제가 있다. 2개의 단어가 함께 등장하는 횟수가 0일 때, log2 0는 음의 무한대이기 때문이다. 따라서, 실제로는 PPMI(Positive PMI)라는 수치를 사용한다. PPMI는 PMI가 -값이 되면 0으로 계산하여, 단어 간의 관련성을 0 이상의 실수로 표시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(C, verbose=False, eps=1e-8):\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose: # 진행상황을 출력할거니?\n",
    "                cnt += 1\n",
    "                if cnt % (total // 100) == 0:\n",
    "                    print('%.1f%% done' % (100*cnt/total))\n",
    "    \n",
    "    return M "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "covariance matrix\n[[0 1 0 0 0 0 0]\n [1 0 1 0 1 1 0]\n [0 1 0 1 0 0 0]\n [0 0 1 0 1 0 0]\n [0 1 0 1 0 0 0]\n [0 1 0 0 0 0 1]\n [0 0 0 0 0 1 0]]\n--------------------------------------------------\nPPMI\n[[0.    1.253 0.    0.    0.    0.    0.   ]\n [1.253 0.    0.56  0.    0.56  0.56  0.   ]\n [0.    0.56  0.    1.253 0.    0.    0.   ]\n [0.    0.    1.253 0.    1.253 0.    0.   ]\n [0.    0.56  0.    1.253 0.    0.    0.   ]\n [0.    0.56  0.    0.    0.    0.    1.946]\n [0.    0.    0.    0.    0.    1.946 0.   ]]\n"
    }
   ],
   "source": [
    "text = 'you say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print('covariance matrix')\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 PPMI에도 문제가 있다. 그것은 코퍼스의 어휘량이 증가할수록 각 단어 벡터의 차원수가 늘어난다는 것이다. 10만 단어가 있다면...? 벡터의 차원수도 10만 차원이 된다. NO WAY!\n",
    "\n",
    "또한 행렬 원소 중 0이 많은 것을 알 수 있다. 이 벡터는 노이즈에 약하다는 문제가 있다. 이런 문제에 대해서는 '차원을 줄이는 것'으로 대응한다.\n",
    "\n",
    "### Dimensionality Reduction\n",
    "벡터의 차원을 감소시키자! 다만, 중요한 정보는 남기면서 줄이는 게 포인트이다. 차원을 줄이는 데는 특이값분해(SVD)가 많이 사용된다. SVD는 임의의 행렬을 3가지의 행렬의 내적으로 분해한다. SVD = USV.T에서 U, V는 직교행렬, S는 대각행렬이다. S는 대각행렬이므로 대각성분 이외에는 모두 0인데, 이 대각성분에는 '특이값'이라는 것이 큰 순서대로 늘어서 있다. 여기서 작은 특이값은 의미가 없으므로 행렬 자체의 크기를 줄여버리자는 것이 SVD의 핵심이다.\n",
    "\n",
    "결국, S를 줄이면 U도 줄어들게 되어, U는 차원이 감소한 단어벡터가 된다.\n",
    "\n",
    "파이썬으로 SVD를 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 1 0 0 0 0 0]\n[0.    1.253 0.    0.    0.    0.    0.   ]\n[-1.110e-16  3.409e-01  1.205e-01 -7.772e-16  0.000e+00  9.323e-01\n -4.384e-17]\n"
    }
   ],
   "source": [
    "U, S, V = np.linalg.svd(W) # W는 위 코드에서 볼 수 있듯, PPMI 값\n",
    "\n",
    "print(C[0]) # cocurrence matrix\n",
    "print(W[0]) # PPMI Matrix\n",
    "print(U[0]) # SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 알 수 있듯, 0이 가득했던 sparse vector `W[0]`가 특이값분해 이후 값이 모두 존재하는 벡터 `U[0]`으로 변환된다. 이 벡터의 차원을 줄이기 위해서는, 단순히 앞의 N개만을 갖고오면 된다. 2차원의 벡터로 감소시켜 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-1.110e-16  3.409e-01]\n"
    }
   ],
   "source": [
    "print(U[0, :2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 단어를 2차원의 벡터로 표시하여, 그래프에 나타내 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "\n",
    "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD의 계산은 O(N**3)의 복잡도를 자랑하므로, 보통은 특이값이 작은 것은 모두 버리는 Truncated SVD를 사용한다.\n",
    "\n",
    "이제 PTB 데이터셋을 사용해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "corpus size:  929589\ncorpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29]\n\nid_to_word[0] aer\nid_to_word[1] banknote\nid_to_word[2] berlitz\n\nword_to_id[\"car\"] 3856\nword_to_id[\"apple\"] 1410\nword_to_id[\"lexus\"] 7426\n"
    }
   ],
   "source": [
    "# Penn Treebank\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(\".\")\n",
    "\n",
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "print('corpus size: ', len(corpus))\n",
    "print('corpus[:30]:', corpus[:30])\n",
    "print()\n",
    "print('id_to_word[0]', id_to_word[0])\n",
    "print('id_to_word[1]', id_to_word[1])\n",
    "print('id_to_word[2]', id_to_word[2])\n",
    "print()\n",
    "print('word_to_id[\"car\"]', word_to_id['car'])\n",
    "print('word_to_id[\"apple\"]', word_to_id['apple'])\n",
    "print('word_to_id[\"lexus\"]', word_to_id['lexus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "counting co-occurence...\ncalculating PPMI...\n1.0% done\n2.0% done\n3.0% done\n4.0% done\n5.0% done\n6.0% done\n7.0% done\n8.0% done\n9.0% done\n10.0% done\n11.0% done\n12.0% done\n13.0% done\n14.0% done\n15.0% done\n16.0% done\n17.0% done\n18.0% done\n19.0% done\n20.0% done\n21.0% done\n22.0% done\n23.0% done\n24.0% done\n25.0% done\n26.0% done\n27.0% done\n28.0% done\n29.0% done\n30.0% done\n31.0% done\n32.0% done\n33.0% done\n34.0% done\n35.0% done\n36.0% done\n37.0% done\n38.0% done\n39.0% done\n40.0% done\n41.0% done\n42.0% done\n43.0% done\n44.0% done\n45.0% done\n46.0% done\n47.0% done\n48.0% done\n49.0% done\n50.0% done\n51.0% done\n52.0% done\n53.0% done\n54.0% done\n55.0% done\n56.0% done\n57.0% done\n58.0% done\n59.0% done\n60.0% done\n61.0% done\n62.0% done\n63.0% done\n64.0% done\n65.0% done\n66.0% done\n67.0% done\n68.0% done\n69.0% done\n70.0% done\n71.0% done\n72.0% done\n73.0% done\n74.0% done\n75.0% done\n76.0% done\n77.0% done\n78.0% done\n79.0% done\n80.0% done\n81.0% done\n82.0% done\n83.0% done\n84.0% done\n85.0% done\n86.0% done\n87.0% done\n88.0% done\n89.0% done\n90.0% done\n91.0% done\n92.0% done\n93.0% done\n94.0% done\n95.0% done\n96.0% done\n97.0% done\n98.0% done\n99.0% done\n100.0% done\ncalculatin SVD...\n"
    }
   ],
   "source": [
    "# PTB 데이터셋으로 평가\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('counting co-occurence...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print('calculating PPMI...')\n",
    "W = ppmi(C, verbose=True)\n",
    "\n",
    "print('calculatin SVD...')\n",
    "try:\n",
    "    # truncated SVD\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, random_state=None)\n",
    "except ImportError:\n",
    "    # SVD(SLOW)\n",
    "    U, S, V = np.linalg.svd(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n[query] you\ni: 0.84230\nwe: 0.82731\nthey: 0.70114\nthere: 0.53437\nanybody: 0.52751\n\n[query] year\nweek: 0.81634\nmonth: 0.80834\nday: 0.68472\nquarter: 0.66491\njuly: 0.65993\n\n[query] car\nauto: 0.70513\ntruck: 0.66186\njewelry: 0.59510\nluxury-car: 0.58530\ndisk-drive: 0.58326\n\n[query] toyota\nkuwait: 0.67787\naeroflot: 0.61107\nitel: 0.60487\npakistan: 0.60051\nbullets: 0.57280\n"
    }
   ],
   "source": [
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "queries = ['you', 'year', 'car', 'toyota']\n",
    "for query in queries:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}